{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG 16\n",
    "![](https://junjiwon1031.github.io/assets/vgg16.png)\n",
    "- 목적: 원래는 깊이에 따른 학습률을 연구하는 목적으로 사용됬는데, ImageNet Challenge에서 테스트 정화도 92.7%라는 성과를 내어 딥러닝 분야에서 각광 받음.\n",
    "- 의미: \n",
    "    - VGG: Visual Geometry Group\n",
    "    - 16 : 레이어의 수: 13CNN + 3FCN(레이어의 깊이에 따라 13,19로 나뉨)\n",
    "- 의의: 신경망의 깊이가 깊을수록 accuracy가 높아지는 것을 알수 있음.\n",
    "- 구조:\n",
    "    - 3x3 filter(stride=1)만 사용.\n",
    "    - ReLU\n",
    "    - 2x2 max_pooling(stride=2)\n",
    "    - 13CNN + 3Fully_Connected_Network\n",
    "\n",
    "### layer의 증가(A->E로 layer가 증가)\n",
    "- 여기서 conv1, conv3는 filter의 크기가 각각 1, 3이라는 것을 의미\n",
    "- A:layer(11), B:layer(13), C:layer(16), D:layer(16), E:layer(19)\n",
    "- layer가 19까지인 이유는 해당 실험의 데이터에서는 분류 오차율이 VGG-19에서 수렴했기 때문, 학습 데이터 세트가 더 많다면 더 깊은 모델이 더 유용할 수도 있다.\n",
    "![](https://miro.medium.com/max/1400/1*gU5m4XO2awEM6Zp4DkirFA.png)\n",
    "### filter 3x3만 사용한 의미\n",
    "#### filter 7x7\n",
    "![](https://miro.medium.com/max/1400/1*Cb8p7EzcWYDHUzMBYI-yyw.png)\n",
    "#### filter 3x3\n",
    "![](https://miro.medium.com/max/1400/1*E9DiwjWyLU-aQU-knOtv3g.png)\n",
    "> Stride가 1일 때, 3차례의 3x3 Conv 필터링을 반복한 특징맵은 한 픽셀이 원본 이미지의 7x7 Receptive field의 효과를 볼 수 있다.(깊이의 증가)\n",
    "\n",
    "### 7x7 filter와 3x3filter의 차이\n",
    "> 결정 함수의 비선형성(Relu) 증가\n",
    "    - Relu가 많을 수록 특징 식별성이 높아진다.\n",
    "    - Relu는 conv로 인한 이미지 뭉개짐을 풀어주어 특징을 도드라게 해주는 기능을 가지고 있다.\n",
    "> 학습 파라미터 수의 감소\n",
    "\n",
    "    \n",
    "### 결론\n",
    "VGG 연구팀의 실험 결과를 통해 **네트워크의 깊이가 깊어질수록 이미지 분류 정확도가 높아지는 것**을 확인할 수 있었다.\n",
    "하지만 그 모델이 그 데이터에 대해서 적합한 상태에서 깊이를 더 올리면, feature에 대한 특색이 무뎌져 오히려 학습이 잘 안될수가 있다. 따라서, 적당한 길이의 깊이로 해야한다. \n",
    "\n",
    "실험에서 네트워크의 깊이를 최대 19 레이어(VGG-19)까지만 사용한 이유는 해당 실험의 데이터에서는 분류 오차율이 VGG-19에서 수렴했기 때문이다. 학습 데이터 세트가 충분히 많다면 더 깊은 모델이 더 유용할 수도 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile Net\n",
    "\n",
    "### 1.1. 고성능의 환경\n",
    "데이터 센터 환경에서는 고성능의 CPU가 매우 많이 있기 때문에 연산 처리 성능이 매우 고성능이고, 고성능의 그래픽 카드 다수를 가지고 있을 수 있으며, 메모리도 대용량을 사용할 수 있습니다. 또, 전력 공급이 지속적으로 이루어지기 때문에, 전원을 아끼기 위해 성능을 낮추는 것 보다는 더 높은 퍼포먼스에 초점을 두는 경우가 많습니다.\n",
    "\n",
    "### 1.2. 고성능이 아닌 환경\n",
    "이런 고성능의 디바이스가 아니라 자동차, 드론, 스마트폰과 같은 환경에서는 CPU를 하나 정도 가지고 있는 경우도 많고, GPU가 없을 수도 있으며, 메모리도 부족합니다. \n",
    "\n",
    "### 1.3 Mobile Net에 사용된 기술.\n",
    "- Remove Fully-Connected Layers\n",
    "    - CNN에서 파라미터의 90% 정도가 Fully-Connected Layers에 들어가기 때문에, 얘를 줄여야됨!\n",
    "- Kernel Reduction\n",
    "    - 3×3 → 1×1 을 통하여 연산량을 줄인다.\n",
    "- Channel Reduction\n",
    "    - 채널 수를 줄인다.\n",
    "- **Depthwse Separable Convolutions**\n",
    "    - Depthwise Convolution\n",
    "    - Pointwise Convolution\n",
    "    - 연산량: \n",
    "${ filterSize(3x3) * inChannel + filterSize(1x1) * inChannel * outChannel }$\n",
    "${ -> 9 * inChannel + 1 * inChannel * outC\n",
    "hannel }$\n",
    "![](https://cdn-images-1.medium.com/max/1000/1*QsbUwtBTX4GbvUrRUmA7qw.png)\n",
    "\n",
    "### 1.4 Mobile Net 아키텍처\n",
    "![](https://image.slidesharecdn.com/mobilenets-191107205952/95/mobilenet-review-mobile-net-research-paper-review-mobilenet-v1-paper-explained-mobilenet-2017-14-638.jpg?cb=1573160673)\n",
    "\n",
    "#### Inverted Residual\n",
    "![](https://miro.medium.com/max/612/1*BaxdP8RS5x_EVMNJSd1Urg.png)\n",
    "![](https://gaussian37.github.io/assets/img/dl/concept/mobilenet_v2/9.png)\n",
    "- linear_bottleneck: \n",
    "    - 위의 사진의 빗금친 상자로, activation_f가 적용되지 않은 상태를 나타낸다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Better Performance In CNN\n",
    "\n",
    "### Better Network Architecture\n",
    "- Use better model(pretrained) - ex) ImageNet SOTA\n",
    "- More layers(but In Dense_layer Overfitting)\n",
    "- Big Input_size \n",
    "- More channels\n",
    "- Use better activation function - ex) swish, etc..\n",
    "- Use additional architecture - ex) skip connection, SE-module, etc..\n",
    "\n",
    "### Training Trick  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Tricks for Image Classification\n",
    ">## For Better Performance In CNN\n",
    "\n",
    "### Better Network Architecture\n",
    "- Use better model(pretrained) - ex) ImageNet SOTA\n",
    "- More layers(but In Dense_layer Overfitting)\n",
    "- Big Input_size \n",
    "- More channels\n",
    "- Use better activation function - ex) swish, etc..\n",
    "- Use additional architecture - ex) skip connection, SE-module, etc..\n",
    "\n",
    "### Training Trick  \n",
    "- #### Efficient Training\n",
    "    - Linear scaling learning rate\n",
    "        - batch_size expand then learning_rate expand\n",
    "    - Learning rate warm up\n",
    "        \n",
    "    - Zero gamma in batchnorm\n",
    "    - No bias decay\n",
    "        - Not L2 for bias\n",
    "    - Low-precision training\n",
    "        - \n",
    "\n",
    "- #### Training Refinements\n",
    "    - ## Cosine Learning Rate Decay\n",
    "    ![](https://gitcdn.xyz/cdn/Tony607/blog_statics/1d8d1e47d0ed931e6dff481a9639b43840ece866/images/cnn_tricks/cosine_decay.png)\n",
    "    \n",
    "    - ## Label Smoothing\n",
    "    ${ correctAnswer:  { 1- \\beta} }$\n",
    "    \n",
    "    ${ otherwise: {\\beta \\over (N - 1)} }$\n",
    "        - 기존 one_hot 방식은 1,0,0,0,0.. 이러한 방식인데, label smoothing은 정답을 1로 하지\n",
    "    \n",
    "    - ## Knowledge Distillation\n",
    "    ![](https://image.slidesharecdn.com/relationalknowledgedistillation-190311024259/95/relational-knowledge-distillation-7-638.jpg?cb=1552272213)\n",
    "    \n",
    "    - ## Mixup Training \n",
    "    ![](https://hoya012.github.io/assets/img/bag_of_trick/9.PNG)\n",
    "    \n",
    "    - ## Cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FullyConvolutionalNetworks for Segmantic Segmentation\n",
    "-  Semantic Segmentation 모델을 위해 기존에 이미지 분류에서 우수한 성능을 보인 CNN 기반 모델(AlexNet, VGG16, GoogLeNet)을 목적에 맞춰 변형시킨 것이다.\n",
    "\n",
    "- Segmentation의 목적은 **원본 이미지의 각 픽셀에 대해 클래스를 구분**하고 **인스턴스 및 배경을 분할하는 것**으로 위치 정보가 매우 중요하다.\n",
    "\n",
    "\n",
    "- Fully_Connected_Network을 Convolution으로 바꾼 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
